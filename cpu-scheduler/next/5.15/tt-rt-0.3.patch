diff --git a/kernel/sched/bs.c b/kernel/sched/bs.c
index 6cdbcf2aa09d..d6ecfc19098d 100644
--- a/kernel/sched/bs.c
+++ b/kernel/sched/bs.c
@@ -823,7 +823,7 @@ check_preempt_tick(struct cfs_rq *cfs_rq, struct sched_entity *curr)
 	if (next != curr) {
 		cfs_rq->local_cand_hrrn = HRRN_PERCENT(&next->tt_node, sched_clock());
 		__update_candidate(cfs_rq, &next->tt_node);
-		resched_curr(rq_of(cfs_rq));
+		resched_curr_lazy(rq_of(cfs_rq));
 	} else {
 		clear_this_candidate(curr);
 	}
@@ -876,7 +876,7 @@ static void check_preempt_wakeup(struct rq *rq, struct task_struct *p, int wake_
 	return;
 
 preempt:
-	resched_curr(rq);
+	resched_curr_lazy(rq);
 }
 
 #ifdef CONFIG_SMP
@@ -1553,7 +1553,7 @@ static void task_fork_fair(struct task_struct *p)
 		update_curr(cfs_rq);
 
 		if (sysctl_sched_child_runs_first)
-			resched_curr(rq);
+			resched_curr_lazy(rq);
 	}
 
 	rq_unlock(rq, &rf);
diff --git a/kernel/sched/bs.h b/kernel/sched/bs.h
index b3d99cf13576..a464f09133f4 100644
--- a/kernel/sched/bs.h
+++ b/kernel/sched/bs.h
@@ -237,7 +237,7 @@ prio_changed_fair(struct rq *rq, struct task_struct *p, int oldprio)
 	 */
 	if (task_current(rq, p)) {
 		if (p->prio > oldprio)
-			resched_curr(rq);
+			resched_curr_lazy(rq);
 	} else
 		check_preempt_curr(rq, p, 0);
 }
diff --git a/kernel/sched/tt_stats.h b/kernel/sched/tt_stats.h
index 7aa1e8936be4..c12f60830791 100644
--- a/kernel/sched/tt_stats.h
+++ b/kernel/sched/tt_stats.h
@@ -791,10 +791,7 @@ static inline void check_schedstat_required(void)
 			trace_sched_stat_iowait_enabled()  ||
 			trace_sched_stat_blocked_enabled() ||
 			trace_sched_stat_runtime_enabled())  {
-		printk_deferred_once("Scheduler tracepoints stat_sleep, stat_iowait, "
-			     "stat_blocked and stat_runtime require the "
-			     "kernel parameter schedstats=enable or "
-			     "kernel.sched_schedstats=1\n");
+		printk_once("Scheduler tracepoints stat_sleep, stat_iowait, stat_blocked and stat_runtime require the kernel parameter schedstats=enable or kernel.sched_schedstats=1\n");
 	}
 #endif
 }
