diff --git a/kernel/sched/bs.c b/kernel/sched/bs.c
index 758cd410d6e6..1f455ee12b7a 100644
--- a/kernel/sched/bs.c
+++ b/kernel/sched/bs.c
@@ -640,7 +640,7 @@ static void
 check_preempt_tick(struct cfs_rq *cfs_rq, struct sched_entity *curr)
 {
 	if (pick_next_entity(cfs_rq, curr) != curr)
-		resched_curr(rq_of(cfs_rq));
+		resched_curr_lazy(rq_of(cfs_rq));
 }
 
 static void
@@ -688,7 +688,7 @@ static void check_preempt_wakeup(struct rq *rq, struct task_struct *p, int wake_
 	return;
 
 preempt:
-	resched_curr(rq);
+	resched_curr_lazy(rq);
 }
 
 #ifdef CONFIG_SMP
@@ -1115,7 +1115,7 @@ static void task_fork_fair(struct task_struct *p)
 		update_curr(cfs_rq);
 
 		if (sysctl_sched_child_runs_first)
-			resched_curr(rq);
+			resched_curr_lazy(rq);
 	}
 
 	rq_unlock(rq, &rf);
diff --git a/kernel/sched/bs.h b/kernel/sched/bs.h
index d413aad1d0a3..21f2dee6bfa3 100644
--- a/kernel/sched/bs.h
+++ b/kernel/sched/bs.h
@@ -233,7 +233,7 @@ prio_changed_fair(struct rq *rq, struct task_struct *p, int oldprio)
 	 */
 	if (task_current(rq, p)) {
 		if (p->prio > oldprio)
-			resched_curr(rq);
+			resched_curr_lazy(rq);
 	} else
 		check_preempt_curr(rq, p, 0);
 }
diff --git a/kernel/sched/tt_stats.h b/kernel/sched/tt_stats.h
index 7aa1e8936be4..c12f60830791 100644
--- a/kernel/sched/tt_stats.h
+++ b/kernel/sched/tt_stats.h
@@ -791,10 +791,7 @@ static inline void check_schedstat_required(void)
 			trace_sched_stat_iowait_enabled()  ||
 			trace_sched_stat_blocked_enabled() ||
 			trace_sched_stat_runtime_enabled())  {
-		printk_deferred_once("Scheduler tracepoints stat_sleep, stat_iowait, "
-			     "stat_blocked and stat_runtime require the "
-			     "kernel parameter schedstats=enable or "
-			     "kernel.sched_schedstats=1\n");
+		printk_once("Scheduler tracepoints stat_sleep, stat_iowait, stat_blocked and stat_runtime require the kernel parameter schedstats=enable or kernel.sched_schedstats=1\n");
 	}
 #endif
 }
